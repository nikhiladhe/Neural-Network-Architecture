# Neural-Network-Architecture
Investigating neural network architectures and activation functions

In this project various activation functions like ReLu,ELU,Softplus and leaky ReLu activation functions have been investigated across different neural network architectures coded from scratch to allow user to experiment by making user defined changes to the architecture. Also the effect of random weight update schedule on network performance was investigated.

Following are the main files:

main1_Simple FC net.ipynb
main2_FC net with batchnorm and dropout.ipynb
main3_ConvolutionalNetworks.ipynb

Please refer to the project report in 'report.pdf'


